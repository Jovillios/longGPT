{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Attention Mechanisms\n",
    "\n",
    "This notebook demonstrates and visualizes the implementation of:\n",
    "1. DilatedAttention\n",
    "2. MultiheadDilatedAttention\n",
    "\n",
    "We'll create some sample inputs and visualize how the attention patterns work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from einops import rearrange\n",
    "\n",
    "from model import DilatedAttention, MultiheadDilatedAttention, GPTConfig\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Testing DilatedAttention\n",
    "\n",
    "Let's create a small example to visualize how the dilated attention works with different segment lengths and dilation rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample inputs\n",
    "B, T, H, D = 1, 16, 4, 8  # batch, seq_len, heads, dim\n",
    "query = torch.randn(B, T, H, D)\n",
    "key = torch.randn(B, T, H, D)\n",
    "value = torch.randn(B, T, H, D)\n",
    "\n",
    "# Initialize attention with different segment lengths and dilation rates\n",
    "segment_lengths = [4, 4, 4, 4]  # 4 segments of length 4\n",
    "dilation_rates = [1, 2, 4, 8]   # different dilation rates\n",
    "attention = DilatedAttention(segment_lengths, dilation_rates)\n",
    "\n",
    "# Forward pass\n",
    "output = attention(query, key, value)\n",
    "\n",
    "print(f\"Input shape: {query.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Patterns\n",
    "\n",
    "Let's create a function to visualize the attention patterns for each head and dilation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_attention_patterns(query, key, value, attention, title=\"Attention Patterns\"):\n",
    "    # Get attention scores\n",
    "    B, T, H, D = query.shape\n",
    "    num_groups = len(attention.dilation_rates)\n",
    "    \n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(H, 1, figsize=(15, 4*H))\n",
    "    if H == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for h in range(H):\n",
    "        # Calculate attention scores for this head\n",
    "        q = query[0, :, h, :]  # (T, D)\n",
    "        k = key[0, :, h, :]    # (T, D)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(D)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Plot attention pattern\n",
    "        sns.heatmap(attn.detach().numpy(), \n",
    "                   ax=axes[h],\n",
    "                   cmap='viridis',\n",
    "                   xticklabels=True,\n",
    "                   yticklabels=True)\n",
    "        axes[h].set_title(f\"Head {h} (Dilation Rate: {attention.dilation_rates[h % num_groups]})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention patterns\n",
    "visualize_attention_patterns(query, key, value, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Testing MultiheadDilatedAttention\n",
    "\n",
    "Now let's test the MultiheadDilatedAttention which combines the dilated attention with multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a config for MultiheadDilatedAttention\n",
    "config = GPTConfig(\n",
    "    block_size=16,\n",
    "    n_head=4,\n",
    "    n_embd=32,\n",
    "    dropout=0.0\n",
    ")\n",
    "\n",
    "# Initialize the attention module\n",
    "multihead_attention = MultiheadDilatedAttention(config)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(1, 16, 32)  # (batch, seq_len, n_embd)\n",
    "\n",
    "# Forward pass\n",
    "output = multihead_attention(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multihead Attention Patterns\n",
    "\n",
    "Let's visualize how the attention is distributed across different heads in the multihead attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_multihead_attention(x, multihead_attention, title=\"Multihead Attention Patterns\"):\n",
    "    # Get the query, key, value projections\n",
    "    B, T, D = x.shape\n",
    "    q, k, v = multihead_attention.c_attn(x).split(D, dim=2)\n",
    "    \n",
    "    # Reshape for visualization\n",
    "    k = k.view(B, T, multihead_attention.n_head, D // multihead_attention.n_head)\n",
    "    q = q.view(B, T, multihead_attention.n_head, D // multihead_attention.n_head)\n",
    "    \n",
    "    # Create a figure with subplots for each head\n",
    "    fig, axes = plt.subplots(multihead_attention.n_head, 1, figsize=(15, 4*multihead_attention.n_head))\n",
    "    if multihead_attention.n_head == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for h in range(multihead_attention.n_head):\n",
    "        # Calculate attention scores for this head\n",
    "        q_h = q[0, :, h, :]  # (T, D)\n",
    "        k_h = k[0, :, h, :]  # (T, D)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q_h, k_h.transpose(-2, -1)) / np.sqrt(D // multihead_attention.n_head)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Plot attention pattern\n",
    "        sns.heatmap(attn.detach().numpy(), \n",
    "                   ax=axes[h],\n",
    "                   cmap='viridis',\n",
    "                   xticklabels=True,\n",
    "                   yticklabels=True)\n",
    "        axes[h].set_title(f\"Head {h}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize multihead attention patterns\n",
    "visualize_multihead_attention(x, multihead_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with Standard Attention\n",
    "\n",
    "Let's compare the attention patterns between standard attention and our dilated attention implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def standard_attention(query, key, value, scale=None):\n",
    "    \"\"\"Standard scaled dot-product attention\"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    if scale is None:\n",
    "        scale = 1.0 / math.sqrt(d_k)\n",
    "    \n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) * scale\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, value), attn\n",
    "\n",
    "# Create sample inputs\n",
    "B, T, H, D = 1, 16, 4, 8\n",
    "query = torch.randn(B, T, H, D)\n",
    "key = torch.randn(B, T, H, D)\n",
    "value = torch.randn(B, T, H, D)\n",
    "\n",
    "# Get attention patterns from both implementations\n",
    "_, standard_attn = standard_attention(query, key, value)\n",
    "dilated_output = attention(query, key, value)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot standard attention\n",
    "sns.heatmap(standard_attn[0, 0].detach().numpy(), \n",
    "           ax=ax1,\n",
    "           cmap='viridis',\n",
    "           xticklabels=True,\n",
    "           yticklabels=True)\n",
    "ax1.set_title(\"Standard Attention (Head 0)\")\n",
    "\n",
    "# Plot dilated attention\n",
    "scores = torch.matmul(query[0, 0], key[0, 0].transpose(-2, -1)) / np.sqrt(D)\n",
    "dilated_attn = torch.softmax(scores, dim=-1)\n",
    "sns.heatmap(dilated_attn.detach().numpy(), \n",
    "           ax=ax2,\n",
    "           cmap='viridis',\n",
    "           xticklabels=True,\n",
    "           yticklabels=True)\n",
    "ax2.set_title(\"Dilated Attention (Head 0)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
