{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# set workdir to parent directory\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import CausalSelfAttention, GPTConfig\n",
    "\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "# import xformers.ops as xops\n",
    "from einops import rearrange\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedAttention(nn.Module):\n",
    "    \"\"\"Implement dilated, scaled dot product attention with softmax.\n",
    "    Arguments\n",
    "    ---------\n",
    "        softmax_scale: The temperature to use for the softmax attention.\n",
    "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
    "                      runtime)\n",
    "        attention_dropout: The dropout rate to apply to the attention\n",
    "                           (default: 0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        segment_lengths,\n",
    "        dilation_rates,\n",
    "        softmax_scale=None,\n",
    "        attention_dropout=0.0,\n",
    "        op = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if len(segment_lengths) != len(dilation_rates):\n",
    "            raise ValueError(\n",
    "                \"segment_lengths and dilation_rates must have the same length\"\n",
    "            )\n",
    "\n",
    "        self.segment_lengths = segment_lengths\n",
    "        self.dilation_rates = dilation_rates\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.dropout_p = attention_dropout\n",
    "        self.op = op\n",
    "\n",
    "    def forward(\n",
    "        self, query, key, value, is_causal = False\n",
    "    ) :\n",
    "        # Notation:\n",
    "        #   b - batch size\n",
    "        #   n - sequence length\n",
    "        #   h - number of heads\n",
    "        #   d - embedding dimension\n",
    "        #   s - segment length\n",
    "        #   r - dilation rate\n",
    "        #   g - group size (i.e. number of heads per segment length)\n",
    "        #\n",
    "        # Input shape of query, key, value: (b, n, h, d)\n",
    "        b, _, h, _ = query.shape\n",
    "        out = torch.zeros_like(query)\n",
    "\n",
    "        # *** NOTE ***\n",
    "        # The original paper does not describe how to handle the case where\n",
    "        #   h % len(self.segment_lengths) != 0\n",
    "        #\n",
    "        # In my first implementation, I naively assumed (and asserted) that\n",
    "        # 'h % len(self.segment_lengths) == 0', so that I could evenly distribute\n",
    "        # the heads between the different segment lengths. However, it was not\n",
    "        # possible to reproduce the LongNet hyperparameters with that restriction:\n",
    "        #   h=12, segment_lengths=[2048, 4096, 8192, 16384, 32768]\n",
    "        #   h % len(segment_lengths) == 2\n",
    "        #\n",
    "        # For that reason, I have removed the assertion, and instead grouped the heads\n",
    "        # into (potentially) unequally sized groups.  If not perfectly divisible, then\n",
    "        # the first few groups will have an extraattention head.\n",
    "        num_groups = len(self.dilation_rates)\n",
    "        group_sizes = [h // num_groups] * num_groups\n",
    "        for i in range(h % num_groups):\n",
    "            group_sizes[i] += 1\n",
    "\n",
    "        for i, (g, r, s) in enumerate(\n",
    "            zip(group_sizes, self.dilation_rates, self.segment_lengths)\n",
    "        ):\n",
    "            # Split the input sequences into segments of length 'self.segment_length'\n",
    "            q = rearrange(query, \"b (n s) h d -> b n s h d\", s=s)\n",
    "            k = rearrange(key, \"b (n s) h d -> b n s h d\", s=s)\n",
    "            v = rearrange(value, \"b (n s) h d -> b n s h d\", s=s)\n",
    "            # Apply dilation and segment offset\n",
    "            offset = i % r\n",
    "            hmin = i * g\n",
    "            hmax = (i + 1) * g\n",
    "            q = q[:, :, offset::r, hmin:hmax, :]\n",
    "            k = k[:, :, offset::r, hmin:hmax, :]\n",
    "            v = v[:, :, offset::r, hmin:hmax, :]\n",
    "            # Fold all 'n' segments into the batch dimension\n",
    "            q = rearrange(q, \"b n s h d -> (b n) s h d\")\n",
    "            k = rearrange(k, \"b n s h d -> (b n) s h d\")\n",
    "            v = rearrange(v, \"b n s h d -> (b n) s h d\")\n",
    "\n",
    "            # Apply memory efficient attention\n",
    "            # NOTE: If flash attention is correctly installed, then this will also\n",
    "            # automatically use the flash attention implementation.\n",
    "            attn_bias = xops.LowerTriangularMask() if is_causal else None\n",
    "            x = xops.memory_efficient_attention(\n",
    "                query=q, key=k, value=v, op=self.op, attn_bias=attn_bias\n",
    "            )\n",
    "            # Unfold 'n' segments back out of the batch dimension.\n",
    "            x = rearrange(x, \"(b n) s h d -> b n s h d\", b=b)\n",
    "            # Normalize attention outputs across the sequence length dimension. This\n",
    "            # is necessary because the attention outputs from each dilation rate /\n",
    "            # segment length are summed together.\n",
    "            x = x / x.sum(dim=(1, 2), keepdim=True)\n",
    "\n",
    "            # Gather the attention outputs from each dilation rate / segment length.\n",
    "            out = rearrange(out, \"b (n s) h d -> b n s h d\", s=s)\n",
    "            out[:, :, offset::r, hmin:hmax, :] += x\n",
    "            out = rearrange(out, \"b n s h d -> b (n s) h d\", s=s)\n",
    "\n",
    "        # We have already normalized each attention output across the sequence length.\n",
    "        # Now, normalize across all attention outputs by dividing by the number of\n",
    "        # attention groups.  See: https://arxiv.org/pdf/2307.02486.pdf, Eq. 10\n",
    "        return out / num_groups\n",
    "\n",
    "class MultiheadDilatedAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(\n",
    "            config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.attention = DilatedAttention(\n",
    "            segment_lengths=[2048, 4096, 8192, 16384, 32768],\n",
    "            dilation_rates=[1, 2, 4, 6, 12],\n",
    "            attention_dropout=config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, d = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        \n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(b, t, self.n_head, d //\n",
    "                   self.n_head)  # (b, t, h, d)\n",
    "        q = q.view(b, t, self.n_head, d //\n",
    "                   self.n_head)  # (b, t, h, d)\n",
    "        v = v.view(b, t, self.n_head, d //\n",
    "                   self.n_head)  # (b, t, h, d)\n",
    "        y = self.attention(q, k, v, is_causal=True)\n",
    "        y = rearrange(x, 'b t h d -> b t (h d)')\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(b, t, d)\n\u001b[1;32m     13\u001b[0m m \u001b[38;5;241m=\u001b[39m MultiheadDilatedAttention(config)\n\u001b[0;32m---> 14\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 139\u001b[0m, in \u001b[0;36mMultiheadDilatedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    135\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(b, t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, d \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    136\u001b[0m            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)  \u001b[38;5;66;03m# (b, t, h, d)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(b, t, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head, d \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    138\u001b[0m            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head)  \u001b[38;5;66;03m# (b, t, h, d)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m y \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb t h d -> b t (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 89\u001b[0m, in \u001b[0;36mDilatedAttention.forward\u001b[0;34m(self, query, key, value, is_causal)\u001b[0m\n\u001b[1;32m     84\u001b[0m v \u001b[38;5;241m=\u001b[39m rearrange(v, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n s h d -> (b n) s h d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Apply memory efficient attention\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# NOTE: If flash attention is correctly installed, then this will also\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# automatically use the flash attention implementation.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m attn_bias \u001b[38;5;241m=\u001b[39m \u001b[43mxops\u001b[49m\u001b[38;5;241m.\u001b[39mLowerTriangularMask() \u001b[38;5;28;01mif\u001b[39;00m is_causal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m xops\u001b[38;5;241m.\u001b[39mmemory_efficient_attention(\n\u001b[1;32m     91\u001b[0m     query\u001b[38;5;241m=\u001b[39mq, key\u001b[38;5;241m=\u001b[39mk, value\u001b[38;5;241m=\u001b[39mv, op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mop, attn_bias\u001b[38;5;241m=\u001b[39mattn_bias\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Unfold 'n' segments back out of the batch dimension.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xops' is not defined"
     ]
    }
   ],
   "source": [
    "config = GPTConfig(\n",
    "    block_size=16,\n",
    "    vocab_size=50304,\n",
    "    n_layer=12,\n",
    "    n_head=4,\n",
    "    n_embd=16,\n",
    "    dropout=0.1,\n",
    "    bias=True,\n",
    "\n",
    ")\n",
    "b, t, d = 4, 4096, 16\n",
    "x = torch.randn(b, t, d)\n",
    "m = MultiheadDilatedAttention(config)\n",
    "y = m(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
